#!/usr/bin/env python

import requests
from bs4 import BeautifulSoup

def fetch_website(url):
    """
    Fetches the HTML content of the provided URL.
    
    Args:
    url (str): The URL of the website to fetch.
    
    Returns:
    bytes: The HTML content of the website, or None if failed.
    """
    try:
        response = requests.get(url)
        if response.status_code == 200:
            return response.content
        else:
            print("Failed to fetch website. Status code:", response.status_code)
            return None
    except Exception as e:
        print("An error occurred while fetching website:", str(e))
        return None

def parse_html(html_content):
    """
    Parses the HTML content and extracts relevant information.
    
    Args:
    html_content (bytes): The HTML content to parse.
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        # Add your parsing logic here
        # For example:
        # links = soup.find_all('a')
        # for link in links:
        #     print(link.get('href'))
        # Example : Find all anchor tags and print their href attributes
        links = soup.find_all('a')
        for link in links:
            print(link.get('href'))
    except Exception as e:
        print("An error occurred while parsing HTML:", str(e))

def main():
    """
    Main function to execute the web scraping script.
    """
    url = input("Enter the URL to scrape: ")
    html_content = fetch_website(url)
    if html_content:
        parse_html(html_content)

if __name__ == "__main__":
    main()

